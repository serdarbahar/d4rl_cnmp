{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def log_prob_loss(output, y_target): \n",
    "    mean, std = output.chunk(2, dim=-1)\n",
    "    std = F.softplus(std)\n",
    "    dist = D.Normal(loc=mean, scale=std)\n",
    "    return -torch.mean(dist.log_prob(y_target)) \n",
    "\n",
    "class CNMP(nn.Module):\n",
    "    def __init__(self, d_x, d_y, d_SM):\n",
    "        super(CNMP, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(d_x + d_y, 64), nn.LayerNorm(64), nn.ReLU(),\n",
    "            nn.Linear(64, 64), nn.LayerNorm(64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.LayerNorm(128), nn.ReLU(),\n",
    "            nn.Linear(128, 256), nn.LayerNorm(256), nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d_x + (6) + 256, 512), nn.LayerNorm(512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.LayerNorm(256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.LayerNorm(256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.LayerNorm(128), nn.ReLU(),\n",
    "            nn.Linear(128, 2 * d_SM)  # Output mean and std for\n",
    "        )\n",
    "    def forward(self, obs, context, mask, x_tar): # obs is (n, d_x + d_y)\n",
    "\n",
    "        r = self.encoder(obs)\n",
    "        masked_r = torch.bmm(mask, r)\n",
    "        masked_r_sum = torch.sum(masked_r, dim=1, keepdim=True)  # (1, 128)\n",
    "        r_avg = masked_r_sum / torch.sum(mask, dim=[1,2], keepdim=True)  # (1, 128)\n",
    "        r_avg = r_avg.repeat(1, x_tar.shape[1], 1)\n",
    "        context = context.unsqueeze(1).repeat(1, x_tar.shape[1], 1)  # (n, 1, 9)\n",
    "        concat = torch.cat((r_avg, context, x_tar), dim=-1)\n",
    "        #concat = torch.cat((r_avg, x_tar), dim=-1)\n",
    "        output = self.decoder(concat) # (2*d_y,)\n",
    "        return output, r_avg\n",
    "\n",
    "# gets random number of random obs. points from a random trajectory. Also gets a \n",
    "# random target (x,y) from the same trajectory\n",
    "def get_training_sample(d_SM, batch_size):\n",
    "\n",
    "    n = np.random.randint(0, OBS_MAX, batch_size) + 1  # number of observations\n",
    "    perm = np.random.permutation(d_N)\n",
    "    d = perm[:batch_size]  # select random trajectories\n",
    "\n",
    "    observations = np.zeros((batch_size, OBS_MAX, d_x + d_y))\n",
    "    context = np.zeros((batch_size, 6))\n",
    "    target_X = np.zeros((batch_size, 1, d_x))\n",
    "    target_Y = np.zeros((batch_size, 1, d_SM))\n",
    "    mask = np.zeros((batch_size, OBS_MAX, OBS_MAX))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        perm = np.random.permutation(time_len)\n",
    "        observations[i,:n[i],:d_x] = X[d[i],perm[:n[i]]]\n",
    "        observations[i,:n[i],d_x:d_x+d_y] = Y[d[i],perm[:n[i]]]\n",
    "        #context[i,:] = np.concat((C[d[i]], O[d[i], perm[n[i]]]), axis=-1)\n",
    "        context[i,:] = C[d[i]]\n",
    "        target_X[i,0] = X[d[i],perm[n[i]]]\n",
    "        target_Y[i,0] = Y[d[i],perm[n[i]],:d_SM]\n",
    "        mask[i,:n[i],:n[i]] = 1\n",
    "    \n",
    "    return torch.from_numpy(observations), torch.from_numpy(context), \\\n",
    "            torch.from_numpy(target_X), torch.from_numpy(target_Y), torch.from_numpy(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action data shape: (6, 200, 8)\n",
      "Iteration 0\n",
      "Iteration 0 - Loss: 1.0905\n",
      "Saving model...\n",
      "Iteration 100 - Loss: -0.4862\n",
      "Saving model...\n",
      "Iteration 300 - Loss: -0.4921\n",
      "Saving model...\n",
      "Iteration 700 - Loss: -0.7625\n",
      "Saving model...\n",
      "Iteration 1000 - Loss: -0.7723\n",
      "Saving model...\n",
      "Iteration 1600 - Loss: -0.8081\n",
      "Saving model...\n",
      "Iteration 1700 - Loss: -0.9459\n",
      "Saving model...\n",
      "Iteration 4000 - Loss: -1.0913\n",
      "Saving model...\n",
      "Iteration 10000\n",
      "Iteration 15100 - Loss: -1.2594\n",
      "Saving model...\n",
      "Iteration 15700 - Loss: -1.4689\n",
      "Saving model...\n",
      "Iteration 16500 - Loss: -1.5937\n",
      "Saving model...\n",
      "Iteration 16600 - Loss: -1.6161\n",
      "Saving model...\n",
      "Iteration 16800 - Loss: -1.9103\n",
      "Saving model...\n",
      "Iteration 17400 - Loss: -1.9248\n",
      "Saving model...\n",
      "Iteration 17600 - Loss: -1.9409\n",
      "Saving model...\n",
      "Iteration 17900 - Loss: -2.0177\n",
      "Saving model...\n",
      "Iteration 18400 - Loss: -2.0301\n",
      "Saving model...\n",
      "Iteration 19000 - Loss: -2.0396\n",
      "Saving model...\n",
      "Iteration 19100 - Loss: -2.3369\n",
      "Saving model...\n",
      "Iteration 20000\n",
      "Iteration 20200 - Loss: -2.3406\n",
      "Saving model...\n",
      "Iteration 22300 - Loss: -2.5056\n",
      "Saving model...\n",
      "Iteration 25600 - Loss: -2.5748\n",
      "Saving model...\n",
      "Iteration 28000 - Loss: -2.6490\n",
      "Saving model...\n",
      "Iteration 29700 - Loss: -2.7125\n",
      "Saving model...\n",
      "Iteration 30000\n",
      "Iteration 32700 - Loss: -2.7910\n",
      "Saving model...\n",
      "Iteration 39300 - Loss: -2.8426\n",
      "Saving model...\n",
      "Iteration 40000\n",
      "Iteration 50000\n",
      "Iteration 56100 - Loss: -2.8723\n",
      "Saving model...\n",
      "Iteration 60000\n",
      "Iteration 70000\n",
      "Iteration 80000\n",
      "Iteration 90000\n",
      "Iteration 91800 - Loss: -2.8725\n",
      "Saving model...\n",
      "Iteration 100000\n",
      "Iteration 107300 - Loss: -2.8995\n",
      "Saving model...\n",
      "Iteration 110000\n",
      "Iteration 120000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m loss = log_prob_loss(output, y_tar)\n\u001b[32m     67\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m10000\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mIteration \u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28mstr\u001b[39m(i))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aist/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aist/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aist/lib/python3.11/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aist/lib/python3.11/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aist/lib/python3.11/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aist/lib/python3.11/site-packages/torch/optim/adam.py:527\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    525\u001b[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch.is_complex(params[i]):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import validation\n",
    "import importlib\n",
    "importlib.reload(validation)\n",
    "\n",
    "time_len = 201\n",
    "\n",
    "action_data = np.load('data/reach_arm_actions_8_v5.npy')  # shape (25, 451, 30)\n",
    "observation_data = np.load('data/reach_arm_observations_8_v5.npy')  # shape (25, 451, 42)\n",
    "\n",
    "print('Action data shape:', action_data.shape)\n",
    "\n",
    "train_indices = [0, 1, 2, 3, 4, 5] # use all trajectories for training\n",
    "val_indices = []  # use the last trajectory for validation\n",
    "\n",
    "num_data = len(train_indices)\n",
    "\n",
    "X = np.tile(np.linspace(0, 1, time_len).reshape((1, time_len, 1)), (len(train_indices), 1, 1))  # 25 trajectories\n",
    "Y = np.zeros((len(train_indices), time_len, 8))\n",
    "Y[:, 1:] = action_data[train_indices]\n",
    "C = np.zeros((len(train_indices), 6))\n",
    "for i in range(len(train_indices)):\n",
    "    C[i, :3] = observation_data[train_indices[i], 0, 30:33]\n",
    "    C[i, 3:] = observation_data[train_indices[i], 0, 42:45]  # add the first observation as context\n",
    "\n",
    "VAL_Y = np.zeros((len(val_indices), time_len, 8))\n",
    "VAL_Y[:, 1:] = action_data[val_indices]\n",
    "VAL_C = np.zeros((len(val_indices), 6))\n",
    "for i in range(len(val_indices)):\n",
    "    VAL_C[i, :3] = observation_data[val_indices[i], 0, 30:33]\n",
    "    VAL_C[i, 3:] = observation_data[val_indices[i], 0, 42:45]  # add the first observation as context\n",
    "\n",
    "# normalize Y and C by dimensions\n",
    "for dim in range(Y.shape[-1]):\n",
    "    Y_min = np.min(Y[:, :, dim], axis=(0, 1), keepdims=True)\n",
    "    Y_max = np.max(Y[:, :, dim], axis=(0, 1), keepdims=True)\n",
    "    Y[:, :, dim] = (Y[:, :, dim] - Y_min) / (Y_max - Y_min + 1e-8)\n",
    "    VAL_Y[:, :, dim] = (VAL_Y[:, :, dim] - Y_min) / (Y_max - Y_min + 1e-8)\n",
    "\n",
    "for dim in range(C.shape[-1]):\n",
    "    C_min = np.min(C[:, dim], axis=0, keepdims=True)\n",
    "    C_max = np.max(C[:, dim], axis=0, keepdims=True)\n",
    "    C[:, dim] = (C[:, dim] - C_min) / (C_max - C_min + 1e-8)\n",
    "    VAL_C[:, dim] = (VAL_C[:, dim] - C_min) / (C_max - C_min + 1e-8)\n",
    "\n",
    "OBS_MAX = 10\n",
    "d_x = X.shape[-1]\n",
    "d_y = Y.shape[-1]\n",
    "d_SM = d_y\n",
    "d_N = Y.shape[0]\n",
    "batch_size = 2\n",
    "\n",
    "model = CNMP(d_x, d_y, d_SM).double()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=7.5e-2)\n",
    "\n",
    "losses = []\n",
    "errors = []\n",
    "\n",
    "for i in range(300_000):\n",
    "\n",
    "    obs, context, x_tar, y_tar, mask = get_training_sample(d_SM, batch_size)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output, _ = model(obs, context, mask, x_tar)\n",
    "    loss = log_prob_loss(output, y_tar)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print('Iteration ' + str(i))\n",
    "    if i % 100 == 0:\n",
    "\n",
    "        #epoch_val_error = validation.val(model, VAL_Y, VAL_C, time_len)\n",
    "        #errors.append(epoch_val_error)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if min(losses) == loss.item():\n",
    "            print('Iteration ' + str(i) + ' - Loss: ' + '%.4f' % loss.item()) # + ' - Val Error: ' + '%.4f' % epoch_val_error)\n",
    "            print('Saving model...')\n",
    "            torch.save(model.state_dict(), 'save/best_models_reach_arm_8_v5/model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
